{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š ExploraÃ§Ã£o dos Datasets - DataOps Governance Lab\n",
    "\n",
    "Este notebook demonstra como carregar e explorar os datasets do projeto **TechCommerce** utilizados no curso de DataOps: GovernanÃ§a e Qualidade de Dados.\n",
    "\n",
    "## ğŸ¯ Objetivos\n",
    "- Carregar os 4 datasets principais do projeto\n",
    "- Verificar a estrutura e quantidade de registros\n",
    "- Realizar uma anÃ¡lise inicial dos dados\n",
    "- Identificar possÃ­veis problemas de qualidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š ImportaÃ§Ã£o das Bibliotecas\n",
    "\n",
    "Primeiro, importamos a biblioteca **pandas** que serÃ¡ utilizada para manipulaÃ§Ã£o e anÃ¡lise dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ConfiguraÃ§Ã£o para melhor visualizaÃ§Ã£o dos DataFrames\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Carregamento dos Datasets\n",
    "\n",
    "Os datasets estÃ£o localizados na pasta `datasets/` e representam diferentes aspectos do negÃ³cio da **TechCommerce**:\n",
    "\n",
    "- **clientes.csv**: InformaÃ§Ãµes dos clientes\n",
    "- **produtos.csv**: CatÃ¡logo de produtos\n",
    "- **vendas.csv**: TransaÃ§Ãµes de vendas\n",
    "- **logistica.csv**: Dados de entrega e logÃ­stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o caminho base para os datasets\n",
    "base_path = '../datasets/'\n",
    "\n",
    "# Carregar cada dataset usando pandas\n",
    "clientes = pd.read_csv(f'{base_path}clientes.csv')\n",
    "produtos = pd.read_csv(f'{base_path}produtos.csv')\n",
    "vendas = pd.read_csv(f'{base_path}vendas.csv')\n",
    "logistica = pd.read_csv(f'{base_path}logistica.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š AnÃ¡lise Quantitativa dos Datasets\n",
    "\n",
    "Vamos verificar a quantidade de registros em cada dataset para ter uma visÃ£o geral do volume de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir a quantidade de registros em cada dataset\n",
    "print(f\"ğŸ“‹ Resumo dos Datasets:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Clientes: {len(clientes)} registros\")\n",
    "print(f\"Produtos: {len(produtos)} registros\")\n",
    "print(f\"Vendas: {len(vendas)} registros\")\n",
    "print(f\"LogÃ­stica: {len(logistica)} registros\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total de registros: {len(clientes) + len(produtos) + len(vendas) + len(logistica)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ExploraÃ§Ã£o Detalhada por Dataset\n",
    "\n",
    "Agora vamos analisar cada dataset individualmente para entender sua estrutura e identificar possÃ­veis problemas de qualidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‘¥ Dataset: Clientes\n",
    "\n",
    "ContÃ©m informaÃ§Ãµes dos clientes da TechCommerce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ DATASET: CLIENTES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"DimensÃµes: {clientes.shape}\")\n",
    "print(f\"Colunas: {list(clientes.columns)}\")\n",
    "print(\"\\nğŸ“Š Primeiros 5 registros:\")\n",
    "display(clientes.head())\n",
    "\n",
    "print(\"\\nğŸ” InformaÃ§Ãµes do Dataset:\")\n",
    "clientes.info()\n",
    "\n",
    "print(\"\\nâš ï¸ Valores Nulos:\")\n",
    "print(clientes.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ›ï¸ Dataset: Produtos\n",
    "\n",
    "CatÃ¡logo de produtos disponÃ­veis na loja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ DATASET: PRODUTOS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"DimensÃµes: {produtos.shape}\")\n",
    "print(f\"Colunas: {list(produtos.columns)}\")\n",
    "print(\"\\nğŸ“Š Primeiros 5 registros:\")\n",
    "display(produtos.head())\n",
    "\n",
    "print(\"\\nğŸ” InformaÃ§Ãµes do Dataset:\")\n",
    "produtos.info()\n",
    "\n",
    "print(\"\\nâš ï¸ Valores Nulos:\")\n",
    "print(produtos.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’° Dataset: Vendas\n",
    "\n",
    "TransaÃ§Ãµes de vendas realizadas na plataforma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ DATASET: VENDAS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"DimensÃµes: {vendas.shape}\")\n",
    "print(f\"Colunas: {list(vendas.columns)}\")\n",
    "print(\"\\nğŸ“Š Primeiros 5 registros:\")\n",
    "display(vendas.head())\n",
    "\n",
    "print(\"\\nğŸ” InformaÃ§Ãµes do Dataset:\")\n",
    "vendas.info()\n",
    "\n",
    "print(\"\\nâš ï¸ Valores Nulos:\")\n",
    "print(vendas.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸšš Dataset: LogÃ­stica\n",
    "\n",
    "Dados de entrega e logÃ­stica dos pedidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ DATASET: LOGÃSTICA\")\n",
    "print(\"=\"*50)\n",
    "print(f\"DimensÃµes: {logistica.shape}\")\n",
    "print(f\"Colunas: {list(logistica.columns)}\")\n",
    "print(\"\\nğŸ“Š Primeiros 5 registros:\")\n",
    "display(logistica.head())\n",
    "\n",
    "print(\"\\nğŸ” InformaÃ§Ãµes do Dataset:\")\n",
    "logistica.info()\n",
    "\n",
    "print(\"\\nâš ï¸ Valores Nulos:\")\n",
    "print(logistica.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ AnÃ¡lise Consolidada\n",
    "\n",
    "Vamos criar um resumo consolidado de todos os datasets para ter uma visÃ£o geral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um DataFrame resumo\n",
    "datasets_info = {\n",
    "    'Dataset': ['Clientes', 'Produtos', 'Vendas', 'LogÃ­stica'],\n",
    "    'Registros': [len(clientes), len(produtos), len(vendas), len(logistica)],\n",
    "    'Colunas': [len(clientes.columns), len(produtos.columns), len(vendas.columns), len(logistica.columns)],\n",
    "    'Valores_Nulos': [\n",
    "        clientes.isnull().sum().sum(),\n",
    "        produtos.isnull().sum().sum(),\n",
    "        vendas.isnull().sum().sum(),\n",
    "        logistica.isnull().sum().sum()\n",
    "    ]\n",
    "}\n",
    "\n",
    "resumo_df = pd.DataFrame(datasets_info)\n",
    "\n",
    "print(\"ğŸ“Š RESUMO CONSOLIDADO DOS DATASETS\")\n",
    "print(\"=\"*60)\n",
    "display(resumo_df)\n",
    "\n",
    "# Calcular totais\n",
    "total_registros = resumo_df['Registros'].sum()\n",
    "total_colunas = resumo_df['Colunas'].sum()\n",
    "total_nulos = resumo_df['Valores_Nulos'].sum()\n",
    "\n",
    "print(f\"\\nğŸ“‹ TOTAIS:\")\n",
    "print(f\"Total de registros: {total_registros}\")\n",
    "print(f\"Total de colunas: {total_colunas}\")\n",
    "print(f\"Total de valores nulos: {total_nulos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ PrÃ³ximos Passos\n",
    "\n",
    "Com esta exploraÃ§Ã£o inicial, identificamos:\n",
    "\n",
    "1. **Volume de dados**: 83 registros distribuÃ­dos em 4 datasets\n",
    "2. **Estrutura**: Diferentes esquemas para cada tipo de dado\n",
    "3. **Qualidade**: PresenÃ§a de valores nulos que precisam ser tratados\n",
    "\n",
    "### ğŸ” RecomendaÃ§Ãµes para AnÃ¡lise AvanÃ§ada:\n",
    "\n",
    "- **Implementar validaÃ§Ãµes com Great Expectations**\n",
    "- **Verificar integridade referencial entre datasets**\n",
    "- **Aplicar as 6 dimensÃµes da qualidade de dados**\n",
    "- **Criar pipelines de monitoramento contÃ­nuo**\n",
    "\n",
    "### ğŸ“š Recursos Relacionados:\n",
    "\n",
    "- `Lab_DataOps_Governanca_Qualidade.ipynb` - LaboratÃ³rio completo\n",
    "- `Desafio_Final_DataOps.md` - Desafio prÃ¡tico\n",
    "- `Conceitos.md` - Fundamentos teÃ³ricos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
